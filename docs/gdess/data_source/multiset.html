<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>gdess.data_source.multiset API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gdess.data_source.multiset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Union
import pickle, logging

import numpy as np
import pandas as pd
import xarray as xr
import matplotlib as mpl
import matplotlib.pyplot as plt

from gdess import set_verbose
from gdess.operations.anomalies import monthly_anomalies
from gdess.operations.datasetdict import DatasetDict
from gdess.operations.time import ensure_datetime64_array, ensure_dataset_datetime64

_multiset_logger = logging.getLogger(&#34;{0}.{1}&#34;.format(__name__, &#34;multiset&#34;))


class Multiset:
    &#34;&#34;&#34;Work simultaneous with multiple, consistent xarray Datasets.&#34;&#34;&#34;

    def __init__(self, verbose: Union[bool, str] = False):
        &#34;&#34;&#34;A template against which we can run recipes, with an order of operations:
            - Step A: datasets loaded, in their original form
            - Step B: datasets that have been preprocessed
            - Step C: datasets that have operations lazily queued or fully processed

        Parameters
        ----------
        verbose : Union[bool, str], default False
            either True, False, or a string for level such as &#34;INFO, DEBUG, etc.&#34;
        &#34;&#34;&#34;
        self.stepA_original_datasets: Union[DatasetDict, None] = None
        self.stepB_preprocessed_datasets: Union[DatasetDict, None] = None
        self.stepC_prepped_datasets: DatasetDict = DatasetDict(dict())

        set_verbose(_multiset_logger, verbose)

    # def datasets_to_file(self, filename: str = &#39;cmip_collection.latest_executed_datasets.pickle&#39;,):
    #     &#34;&#34;&#34;Pickle the latest executed dataset dictionary using the highest protocol available.
    #
    #     Parameters
    #     ----------
    #     filename
    #
    #     &#34;&#34;&#34;
    #     with open(filename, &#39;wb&#39;) as f:
    #         pickle.dump(self.latest_executed_datasets, f, pickle.HIGHEST_PROTOCOL)

    def datasets_from_pickle(self,
                             filename: str = &#39;cmip_collection.latest_executed_datasets.pickle&#39;,
                             replace: bool = False) -&gt; Union[bool, DatasetDict]:
        &#34;&#34;&#34;Load a dataset dictionary from a saved pickle file.

        Parameters
        ----------
        filename : str, default &#39;cmip_collection.latest_executed_datasets.pickle&#39;
        replace : bool, default False

        Returns
        -------
        If loaded successfully:
            True, if replace==True
            A DatasetDict if replace==False
        if loaded unsuccessfully:
            False

        Notes
        -----
        The pickle load protocol version used is detected automatically, so we do not have to specify it.
        &#34;&#34;&#34;
        if not filename:
            return False

        with open(filename, &#39;rb&#39;) as f:
            _multiset_logger.debug(&#39;Loading dataset from file..&#39;)
            if replace:
                self.stepC_prepped_datasets = pickle.load(f)
                return True
            else:
                return pickle.load(f)

    def validate_time_options(self, starttime_option, endtime_option) -&gt; None:
        &#34;&#34;&#34;Check whether the specified start time is before the data&#39;s end time
            and the specified end time is after the data&#39;s start time.

        Parameters
        ----------
        starttime_option
        endtime_option

        Raises
        ------
        ValueError
        &#34;&#34;&#34;
        for k, v in self.stepB_preprocessed_datasets.items():
            data_starttime = v[&#39;time&#39;].min().values
            data_endtime = v[&#39;time&#39;].max().values
            if starttime_option &gt; data_endtime:
                raise ValueError(&#34;The specified start time (%s) is later than %s data&#39;s end time (%s)&#34;,
                                 starttime_option, k, data_endtime)
            elif endtime_option &lt; data_starttime:
                raise ValueError(&#34;The specified end time (%s) is later than %s data&#39;s start time (%s)&#34;,
                                 endtime_option, k, data_starttime)

    @staticmethod
    def get_anomaly_dataframes(data: Union[xr.DataArray, xr.Dataset],
                               varname: str
                               ) -&gt; (pd.DataFrame, pd.DataFrame):
        &#34;&#34;&#34;

        Parameters
        ----------
        data : Union[xr.DataArray, xr.Dataset]
        varname : str

        Raises
        ------
        TypeError

        Returns
        -------
        tuple
            A pandas Dataframe
            A pandas Dataframe
        &#34;&#34;&#34;
        if isinstance(data, xr.DataArray):
            data = ensure_datetime64_array(data)
        elif isinstance(data, xr.Dataset):
            data = ensure_dataset_datetime64(data)
        else:
            raise TypeError(&#39;Unexpected type &lt;%s&gt;. Was expecting either xarray.Dataset or xarray.DataArray&#39;,
                            type(data))

        # Calculate
        df_anomaly = monthly_anomalies(data, varname=varname)
        # Reformat data structures for plotting
        _df_anomaly_yearly = df_anomaly.pivot(index=&#39;moy&#39;, columns=&#39;year&#39;, values=&#39;monthly_anomaly_from_year&#39;)
        _df_anomaly_mean_cycle = df_anomaly.groupby(&#39;moy&#39;).mean().reset_index()

        return _df_anomaly_mean_cycle, _df_anomaly_yearly

    @staticmethod
    def categorical_cmap(nc: int,
                         nsc: int,
                         cmap: str = &#34;tab10&#34;,
                         continuous: bool = False
                         ) -&gt; mpl.colors.ListedColormap:
        &#34;&#34;&#34;
        Parameters
        ----------
        nc : int
            number of categories
        nsc : int
            number of subcategories
        cmap : str, default &#39;tab10&#39;
        continuous : bool, default False

        Returns
        -------
            A colormap with nc*nsc different colors, where for each category there are nsc colors of same hue

        Notes
        -----
        matplotlib.colors.ListedColormap
            from https://stackoverflow.com/questions/47222585/matplotlib-generic-colormap-from-tab10
        &#34;&#34;&#34;
        if nc &gt; plt.get_cmap(cmap).N:
            raise ValueError(&#34;Too many categories for colormap.&#34;)
        if continuous:
            ccolors = plt.get_cmap(cmap)(np.linspace(0, 1, nc))
        else:
            ccolors = plt.get_cmap(cmap)(np.arange(nc, dtype=int))
        cols = np.zeros((nc * nsc, 3))
        for i, c in enumerate(ccolors):
            chsv = mpl.colors.rgb_to_hsv(c[:3])
            arhsv = np.tile(chsv, nsc).reshape(nsc, 3)
            arhsv[:, 1] = np.linspace(chsv[1], 0.25, nsc)
            arhsv[:, 2] = np.linspace(chsv[2], 1, nsc)
            rgb = mpl.colors.hsv_to_rgb(arhsv)
            cols[i * nsc:(i + 1) * nsc, :] = rgb
        cmap = mpl.colors.ListedColormap(cols)
        return cmap

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Build a string representation of the Multiset object&#34;&#34;&#34;
        strrep = f&#34;Multiset: \n&#34; + \
                 self._original_datasets_list_str() + \
                 f&#34;\n&#34; \
                 f&#34;\t all attributes:%s&#34; % &#39;\n\t\t\t&#39;.join(self._obj_attributes_list_str())

        return strrep

    def _obj_attributes_list_str(self) -&gt; list:
        &#34;&#34;&#34;Get a list of each dataset attribute (with &#34;empty&#34; markers)

        Returns
        -------
        list
        &#34;&#34;&#34;
        list_builder = []
        for k in self.__dict__.keys():
            if not k.startswith(&#39;_&#39;):
                if isinstance(self.__dict__[k], pd.DataFrame) | isinstance(self.__dict__[k], pd.Series):
                    # Pandas object truth value can&#39;t be compared without .empty
                    if not self.__dict__[k].empty:
                        list_builder.append(f&#34;{k}: empty&#34;)
                    else:
                        list_builder.append(k)
                elif not self.__dict__[k]:
                    list_builder.append(f&#34;{k}: empty&#34;)
                else:
                    list_builder.append(f&#34;{k}: {type(k)}&#34;)

        return sorted(list_builder)

    def _original_datasets_list_str(self) -&gt; str:
        &#34;&#34;&#34;Get a comma-separated string that lists the identifying keys for each dataset

        Returns
        -------
        str
        &#34;&#34;&#34;
        if self.stepA_original_datasets:
            return &#39;\n\t&#39;.join(self.stepA_original_datasets.keys())
        else:
            return &#39;&#39;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gdess.data_source.multiset.Multiset"><code class="flex name class">
<span>class <span class="ident">Multiset</span></span>
<span>(</span><span>verbose: Union[bool, str] = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Work simultaneous with multiple, consistent xarray Datasets.</p>
<p>A template against which we can run recipes, with an order of operations:
- Step A: datasets loaded, in their original form
- Step B: datasets that have been preprocessed
- Step C: datasets that have operations lazily queued or fully processed</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>verbose</code></strong> :&ensp;<code>Union[bool, str]</code>, default <code>False</code></dt>
<dd>either True, False, or a string for level such as "INFO, DEBUG, etc."</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Multiset:
    &#34;&#34;&#34;Work simultaneous with multiple, consistent xarray Datasets.&#34;&#34;&#34;

    def __init__(self, verbose: Union[bool, str] = False):
        &#34;&#34;&#34;A template against which we can run recipes, with an order of operations:
            - Step A: datasets loaded, in their original form
            - Step B: datasets that have been preprocessed
            - Step C: datasets that have operations lazily queued or fully processed

        Parameters
        ----------
        verbose : Union[bool, str], default False
            either True, False, or a string for level such as &#34;INFO, DEBUG, etc.&#34;
        &#34;&#34;&#34;
        self.stepA_original_datasets: Union[DatasetDict, None] = None
        self.stepB_preprocessed_datasets: Union[DatasetDict, None] = None
        self.stepC_prepped_datasets: DatasetDict = DatasetDict(dict())

        set_verbose(_multiset_logger, verbose)

    # def datasets_to_file(self, filename: str = &#39;cmip_collection.latest_executed_datasets.pickle&#39;,):
    #     &#34;&#34;&#34;Pickle the latest executed dataset dictionary using the highest protocol available.
    #
    #     Parameters
    #     ----------
    #     filename
    #
    #     &#34;&#34;&#34;
    #     with open(filename, &#39;wb&#39;) as f:
    #         pickle.dump(self.latest_executed_datasets, f, pickle.HIGHEST_PROTOCOL)

    def datasets_from_pickle(self,
                             filename: str = &#39;cmip_collection.latest_executed_datasets.pickle&#39;,
                             replace: bool = False) -&gt; Union[bool, DatasetDict]:
        &#34;&#34;&#34;Load a dataset dictionary from a saved pickle file.

        Parameters
        ----------
        filename : str, default &#39;cmip_collection.latest_executed_datasets.pickle&#39;
        replace : bool, default False

        Returns
        -------
        If loaded successfully:
            True, if replace==True
            A DatasetDict if replace==False
        if loaded unsuccessfully:
            False

        Notes
        -----
        The pickle load protocol version used is detected automatically, so we do not have to specify it.
        &#34;&#34;&#34;
        if not filename:
            return False

        with open(filename, &#39;rb&#39;) as f:
            _multiset_logger.debug(&#39;Loading dataset from file..&#39;)
            if replace:
                self.stepC_prepped_datasets = pickle.load(f)
                return True
            else:
                return pickle.load(f)

    def validate_time_options(self, starttime_option, endtime_option) -&gt; None:
        &#34;&#34;&#34;Check whether the specified start time is before the data&#39;s end time
            and the specified end time is after the data&#39;s start time.

        Parameters
        ----------
        starttime_option
        endtime_option

        Raises
        ------
        ValueError
        &#34;&#34;&#34;
        for k, v in self.stepB_preprocessed_datasets.items():
            data_starttime = v[&#39;time&#39;].min().values
            data_endtime = v[&#39;time&#39;].max().values
            if starttime_option &gt; data_endtime:
                raise ValueError(&#34;The specified start time (%s) is later than %s data&#39;s end time (%s)&#34;,
                                 starttime_option, k, data_endtime)
            elif endtime_option &lt; data_starttime:
                raise ValueError(&#34;The specified end time (%s) is later than %s data&#39;s start time (%s)&#34;,
                                 endtime_option, k, data_starttime)

    @staticmethod
    def get_anomaly_dataframes(data: Union[xr.DataArray, xr.Dataset],
                               varname: str
                               ) -&gt; (pd.DataFrame, pd.DataFrame):
        &#34;&#34;&#34;

        Parameters
        ----------
        data : Union[xr.DataArray, xr.Dataset]
        varname : str

        Raises
        ------
        TypeError

        Returns
        -------
        tuple
            A pandas Dataframe
            A pandas Dataframe
        &#34;&#34;&#34;
        if isinstance(data, xr.DataArray):
            data = ensure_datetime64_array(data)
        elif isinstance(data, xr.Dataset):
            data = ensure_dataset_datetime64(data)
        else:
            raise TypeError(&#39;Unexpected type &lt;%s&gt;. Was expecting either xarray.Dataset or xarray.DataArray&#39;,
                            type(data))

        # Calculate
        df_anomaly = monthly_anomalies(data, varname=varname)
        # Reformat data structures for plotting
        _df_anomaly_yearly = df_anomaly.pivot(index=&#39;moy&#39;, columns=&#39;year&#39;, values=&#39;monthly_anomaly_from_year&#39;)
        _df_anomaly_mean_cycle = df_anomaly.groupby(&#39;moy&#39;).mean().reset_index()

        return _df_anomaly_mean_cycle, _df_anomaly_yearly

    @staticmethod
    def categorical_cmap(nc: int,
                         nsc: int,
                         cmap: str = &#34;tab10&#34;,
                         continuous: bool = False
                         ) -&gt; mpl.colors.ListedColormap:
        &#34;&#34;&#34;
        Parameters
        ----------
        nc : int
            number of categories
        nsc : int
            number of subcategories
        cmap : str, default &#39;tab10&#39;
        continuous : bool, default False

        Returns
        -------
            A colormap with nc*nsc different colors, where for each category there are nsc colors of same hue

        Notes
        -----
        matplotlib.colors.ListedColormap
            from https://stackoverflow.com/questions/47222585/matplotlib-generic-colormap-from-tab10
        &#34;&#34;&#34;
        if nc &gt; plt.get_cmap(cmap).N:
            raise ValueError(&#34;Too many categories for colormap.&#34;)
        if continuous:
            ccolors = plt.get_cmap(cmap)(np.linspace(0, 1, nc))
        else:
            ccolors = plt.get_cmap(cmap)(np.arange(nc, dtype=int))
        cols = np.zeros((nc * nsc, 3))
        for i, c in enumerate(ccolors):
            chsv = mpl.colors.rgb_to_hsv(c[:3])
            arhsv = np.tile(chsv, nsc).reshape(nsc, 3)
            arhsv[:, 1] = np.linspace(chsv[1], 0.25, nsc)
            arhsv[:, 2] = np.linspace(chsv[2], 1, nsc)
            rgb = mpl.colors.hsv_to_rgb(arhsv)
            cols[i * nsc:(i + 1) * nsc, :] = rgb
        cmap = mpl.colors.ListedColormap(cols)
        return cmap

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Build a string representation of the Multiset object&#34;&#34;&#34;
        strrep = f&#34;Multiset: \n&#34; + \
                 self._original_datasets_list_str() + \
                 f&#34;\n&#34; \
                 f&#34;\t all attributes:%s&#34; % &#39;\n\t\t\t&#39;.join(self._obj_attributes_list_str())

        return strrep

    def _obj_attributes_list_str(self) -&gt; list:
        &#34;&#34;&#34;Get a list of each dataset attribute (with &#34;empty&#34; markers)

        Returns
        -------
        list
        &#34;&#34;&#34;
        list_builder = []
        for k in self.__dict__.keys():
            if not k.startswith(&#39;_&#39;):
                if isinstance(self.__dict__[k], pd.DataFrame) | isinstance(self.__dict__[k], pd.Series):
                    # Pandas object truth value can&#39;t be compared without .empty
                    if not self.__dict__[k].empty:
                        list_builder.append(f&#34;{k}: empty&#34;)
                    else:
                        list_builder.append(k)
                elif not self.__dict__[k]:
                    list_builder.append(f&#34;{k}: empty&#34;)
                else:
                    list_builder.append(f&#34;{k}: {type(k)}&#34;)

        return sorted(list_builder)

    def _original_datasets_list_str(self) -&gt; str:
        &#34;&#34;&#34;Get a comma-separated string that lists the identifying keys for each dataset

        Returns
        -------
        str
        &#34;&#34;&#34;
        if self.stepA_original_datasets:
            return &#39;\n\t&#39;.join(self.stepA_original_datasets.keys())
        else:
            return &#39;&#39;</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="gdess.data_source.models.cmip.cmip_collection.Collection" href="models/cmip/cmip_collection.html#gdess.data_source.models.cmip.cmip_collection.Collection">Collection</a></li>
<li><a title="gdess.data_source.models.e3sm.collection.Collection" href="models/e3sm/collection.html#gdess.data_source.models.e3sm.collection.Collection">Collection</a></li>
<li><a title="gdess.data_source.observations.gvplus_surface.Collection" href="observations/gvplus_surface.html#gdess.data_source.observations.gvplus_surface.Collection">Collection</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="gdess.data_source.multiset.Multiset.categorical_cmap"><code class="name flex">
<span>def <span class="ident">categorical_cmap</span></span>(<span>nc: int, nsc: int, cmap: str = 'tab10', continuous: bool = False) -> matplotlib.colors.ListedColormap</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>nc</code></strong> :&ensp;<code>int</code></dt>
<dd>number of categories</dd>
<dt><strong><code>nsc</code></strong> :&ensp;<code>int</code></dt>
<dd>number of subcategories</dd>
<dt><strong><code>cmap</code></strong> :&ensp;<code>str</code>, default <code>'tab10'</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>continuous</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<pre><code>A colormap with nc*nsc different colors, where for each category there are nsc colors of same hue
</code></pre>
<h2 id="notes">Notes</h2>
<p>matplotlib.colors.ListedColormap
from <a href="https://stackoverflow.com/questions/47222585/matplotlib-generic-colormap-from-tab10">https://stackoverflow.com/questions/47222585/matplotlib-generic-colormap-from-tab10</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def categorical_cmap(nc: int,
                     nsc: int,
                     cmap: str = &#34;tab10&#34;,
                     continuous: bool = False
                     ) -&gt; mpl.colors.ListedColormap:
    &#34;&#34;&#34;
    Parameters
    ----------
    nc : int
        number of categories
    nsc : int
        number of subcategories
    cmap : str, default &#39;tab10&#39;
    continuous : bool, default False

    Returns
    -------
        A colormap with nc*nsc different colors, where for each category there are nsc colors of same hue

    Notes
    -----
    matplotlib.colors.ListedColormap
        from https://stackoverflow.com/questions/47222585/matplotlib-generic-colormap-from-tab10
    &#34;&#34;&#34;
    if nc &gt; plt.get_cmap(cmap).N:
        raise ValueError(&#34;Too many categories for colormap.&#34;)
    if continuous:
        ccolors = plt.get_cmap(cmap)(np.linspace(0, 1, nc))
    else:
        ccolors = plt.get_cmap(cmap)(np.arange(nc, dtype=int))
    cols = np.zeros((nc * nsc, 3))
    for i, c in enumerate(ccolors):
        chsv = mpl.colors.rgb_to_hsv(c[:3])
        arhsv = np.tile(chsv, nsc).reshape(nsc, 3)
        arhsv[:, 1] = np.linspace(chsv[1], 0.25, nsc)
        arhsv[:, 2] = np.linspace(chsv[2], 1, nsc)
        rgb = mpl.colors.hsv_to_rgb(arhsv)
        cols[i * nsc:(i + 1) * nsc, :] = rgb
    cmap = mpl.colors.ListedColormap(cols)
    return cmap</code></pre>
</details>
</dd>
<dt id="gdess.data_source.multiset.Multiset.get_anomaly_dataframes"><code class="name flex">
<span>def <span class="ident">get_anomaly_dataframes</span></span>(<span>data: Union[xarray.core.dataarray.DataArray, xarray.core.dataset.Dataset], varname: str) -> (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Union[xr.DataArray, xr.Dataset]</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>varname</code></strong> :&ensp;<code>str</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A pandas Dataframe
A pandas Dataframe</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_anomaly_dataframes(data: Union[xr.DataArray, xr.Dataset],
                           varname: str
                           ) -&gt; (pd.DataFrame, pd.DataFrame):
    &#34;&#34;&#34;

    Parameters
    ----------
    data : Union[xr.DataArray, xr.Dataset]
    varname : str

    Raises
    ------
    TypeError

    Returns
    -------
    tuple
        A pandas Dataframe
        A pandas Dataframe
    &#34;&#34;&#34;
    if isinstance(data, xr.DataArray):
        data = ensure_datetime64_array(data)
    elif isinstance(data, xr.Dataset):
        data = ensure_dataset_datetime64(data)
    else:
        raise TypeError(&#39;Unexpected type &lt;%s&gt;. Was expecting either xarray.Dataset or xarray.DataArray&#39;,
                        type(data))

    # Calculate
    df_anomaly = monthly_anomalies(data, varname=varname)
    # Reformat data structures for plotting
    _df_anomaly_yearly = df_anomaly.pivot(index=&#39;moy&#39;, columns=&#39;year&#39;, values=&#39;monthly_anomaly_from_year&#39;)
    _df_anomaly_mean_cycle = df_anomaly.groupby(&#39;moy&#39;).mean().reset_index()

    return _df_anomaly_mean_cycle, _df_anomaly_yearly</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="gdess.data_source.multiset.Multiset.datasets_from_pickle"><code class="name flex">
<span>def <span class="ident">datasets_from_pickle</span></span>(<span>self, filename: str = 'cmip_collection.latest_executed_datasets.pickle', replace: bool = False) -> Union[bool, <a title="gdess.operations.datasetdict.DatasetDict" href="../operations/datasetdict.html#gdess.operations.datasetdict.DatasetDict">DatasetDict</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Load a dataset dictionary from a saved pickle file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code>, default <code>'cmip_collection.latest_executed_datasets.pickle'</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>replace</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>If loaded successfully:</code></dt>
<dd>True, if replace==True
A DatasetDict if replace==False</dd>
<dt><code>if loaded unsuccessfully:</code></dt>
<dd>False</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The pickle load protocol version used is detected automatically, so we do not have to specify it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def datasets_from_pickle(self,
                         filename: str = &#39;cmip_collection.latest_executed_datasets.pickle&#39;,
                         replace: bool = False) -&gt; Union[bool, DatasetDict]:
    &#34;&#34;&#34;Load a dataset dictionary from a saved pickle file.

    Parameters
    ----------
    filename : str, default &#39;cmip_collection.latest_executed_datasets.pickle&#39;
    replace : bool, default False

    Returns
    -------
    If loaded successfully:
        True, if replace==True
        A DatasetDict if replace==False
    if loaded unsuccessfully:
        False

    Notes
    -----
    The pickle load protocol version used is detected automatically, so we do not have to specify it.
    &#34;&#34;&#34;
    if not filename:
        return False

    with open(filename, &#39;rb&#39;) as f:
        _multiset_logger.debug(&#39;Loading dataset from file..&#39;)
        if replace:
            self.stepC_prepped_datasets = pickle.load(f)
            return True
        else:
            return pickle.load(f)</code></pre>
</details>
</dd>
<dt id="gdess.data_source.multiset.Multiset.validate_time_options"><code class="name flex">
<span>def <span class="ident">validate_time_options</span></span>(<span>self, starttime_option, endtime_option) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether the specified start time is before the data's end time
and the specified end time is after the data's start time.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>starttime_option</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>endtime_option</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_time_options(self, starttime_option, endtime_option) -&gt; None:
    &#34;&#34;&#34;Check whether the specified start time is before the data&#39;s end time
        and the specified end time is after the data&#39;s start time.

    Parameters
    ----------
    starttime_option
    endtime_option

    Raises
    ------
    ValueError
    &#34;&#34;&#34;
    for k, v in self.stepB_preprocessed_datasets.items():
        data_starttime = v[&#39;time&#39;].min().values
        data_endtime = v[&#39;time&#39;].max().values
        if starttime_option &gt; data_endtime:
            raise ValueError(&#34;The specified start time (%s) is later than %s data&#39;s end time (%s)&#34;,
                             starttime_option, k, data_endtime)
        elif endtime_option &lt; data_starttime:
            raise ValueError(&#34;The specified end time (%s) is later than %s data&#39;s start time (%s)&#34;,
                             endtime_option, k, data_starttime)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gdess.data_source" href="index.html">gdess.data_source</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gdess.data_source.multiset.Multiset" href="#gdess.data_source.multiset.Multiset">Multiset</a></code></h4>
<ul class="">
<li><code><a title="gdess.data_source.multiset.Multiset.categorical_cmap" href="#gdess.data_source.multiset.Multiset.categorical_cmap">categorical_cmap</a></code></li>
<li><code><a title="gdess.data_source.multiset.Multiset.datasets_from_pickle" href="#gdess.data_source.multiset.Multiset.datasets_from_pickle">datasets_from_pickle</a></code></li>
<li><code><a title="gdess.data_source.multiset.Multiset.get_anomaly_dataframes" href="#gdess.data_source.multiset.Multiset.get_anomaly_dataframes">get_anomaly_dataframes</a></code></li>
<li><code><a title="gdess.data_source.multiset.Multiset.validate_time_options" href="#gdess.data_source.multiset.Multiset.validate_time_options">validate_time_options</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>